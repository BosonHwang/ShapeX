
import os

import numpy as np
import torch
from torch import nn, optim
import random


from tqdm import tqdm
from sklearn.metrics import accuracy_score


from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score
import tsaug

import argparse


# xts_1


#from saliency_exp_synth import get_model, main

import re, time
import argparse
from pathlib import Path
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import trange

from torch.utils.data import DataLoader

from txai.models.encoders.transformer_simple import TransformerMVTS
from txai.models.encoders.simple import CNN, LSTM
from txai.utils.experimental import get_explainer
from txai.vis.vis_saliency import vis_one_saliency
from txai.utils.data import process_Synth
from txai.utils.data.preprocess import process_MITECG, process_Boiler
#from txai.utils.data.anomaly import process_Yahoo
from txai.synth_data.simple_spike import SpikeTrainDataset
from txai.utils.data.preprocess import process_Epilepsy, process_PAM
from txai.synth_data.synth_data_base import SynthTrainDataset


from txai.models.modelv6_v2 import Modelv6_v2
from txai.models.bc_model import TimeXModel
from txai.models.bc_model_irreg import TimeXModel_Irregular


from txai.utils.evaluation import ground_truth_xai_eval
import logging
import sys

from scipy.signal import find_peaks
from utils_training import EarlyStopping,plot_visualize_some,find_project_root
from get_data import get_saliency_data

from PatchTST_supervised.models import ProtoPTST



# Configure logging

#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#print("device", device)

import os



# 当前文件所在的目录
from shapeX import ScoreSubsequences
#from shapex_13_1 import get_seg_pretrain , get_seg_ProtopTST, get_seg_ProtopTST_SNC,ScoreSubsequences
 

## the root dir
root_dir = find_project_root(os.path.dirname(os.path.abspath(__file__)))
print("root dic:", root_dir)

#sys.path.insert(0,"/home/hbs/TS/my_p/shapeX/scr/ShapeX")


#from utils_training import Exp_Basic



def frac_seires(X):
    # X:[T, sample, dim]

    X_frac=X[:,1500:1550,:]
    return X_frac 


def ground_truth_xai_eval_test(generated_exps, gt_exps, penalize_negatives = True, times = None):
    '''
    Compute auprc of generated explanation against ground-truth explanation
        - auprc is computed across one sample, averaged across all samples

    NOTE: Assumes all explanations have batch-first style, i.e. (B,T,d) - captum input/output

    Params:
        generated_exps: Explanations generated by method under evaluation
        gt_exps: Ground-truth explanations on which to evaluate
    '''

    # Normalize generated explanation:
    generated_exps = normalize_exp(generated_exps).detach().clone().cpu().numpy()
    gt_exps = gt_exps.detach().clone().cpu().numpy()
    gt_exps = gt_exps.astype(int)

    all_auprc, all_aup, all_aur = [], [], []
    
    print(generated_exps.shape[1])
    for i in range(generated_exps.shape[1]):
        print("############")
        #auprc = roc_auc_score(gt_exps[i].flatten(), generated_exps[i].flatten())
        # print('gt exps', gt_exps[:,i,:].flatten().shape)
        # print('gen', generated_exps[:,i,:].flatten())
        if times is not None:
            gte = (gt_exps[:,i,:][times[:,i] > -1e5]).flatten()
            gene = normalize_one_exp(generated_exps[:,i,:][times[:,i] > -1e5]).flatten()
            auprc = average_precision_score(gte, gene)
            prec, rec, thres = precision_recall_curve(gte, gene)
            print_nan_positions(auprc)
            
            
        else:
            print("############")
            auprc = average_precision_score(gt_exps[:,i,:].flatten(), generated_exps[:,i,:].flatten())
            
            print_nan_positions(auprc)
            
            prec, rec, thres = precision_recall_curve(gt_exps[:,i,:].flatten(), generated_exps[:,i,:].flatten())
            #print("@@@@@@@@@@@@",prec, "##prec##", rec, "###rec###", thres,"##thres##",gt_exps[:,i,:].flatten().shape)

        aur = auc(thres, rec[:-1]) # Last value in recall curve is always 0 (see sklearn documentation)
        aup = auc(thres, prec[:-1]) # Last value in precision curve is always 1 (see sklearn documentation)
        all_auprc.append(auprc)
        all_aup.append(aup)
        all_aur.append(aur)

    output_dict = {
        'auprc': all_auprc,
        'aup': all_aup,
        'aur': all_aur
    }

    return output_dict

def normalize_exp(exps):
    norm_exps = torch.empty_like(exps)
    for i in range(exps.shape[1]):
        norm_exps[:,i,:] = (exps[:,i,:] - exps[:,i,:].min()) / (exps[:,i,:].max() - exps[:,i,:].min() + 1e-9)
    return norm_exps

def print_nan_positions(array):
    # 创建布尔数组，指示哪些位置是 nan
    nan_mask = np.isnan(array)
    
    # 获取 nan 的位置
    nan_positions = np.where(nan_mask)
    
    # 打印出 nan 的位置
    print("NaN positions:")
    for pos in zip(*nan_positions):
        print(pos)


class Exp_Basic(object):
    def __init__(self, args):
        self.args = args
        self.model_dict = {
            "ProtoPTST":  ProtoPTST, #ProtoPTST
            
        }
        self.device = self._acquire_device()
        self.model = self._build_model().to(self.device)

    def _build_model(self):
        raise NotImplementedError
        return None

    def _acquire_device(self):
        if self.args.use_gpu:
            os.environ["CUDA_VISIBLE_DEVICES"] = (
                str(self.args.gpu) if not self.args.use_multi_gpu else self.args.devices
            )
            device = torch.device("cuda:{}".format(self.args.gpu))
            print("Use GPU: cuda:{}".format(self.args.gpu))
        else:
            device = torch.device("cpu")
            print("Use CPU")
        return device

    def _get_data(self):
        pass

    def vali(self):
        pass

    def train(self):
        pass

    def test(self):
        pass


class Exp_Classification(Exp_Basic):
    def __init__(self, args, data_dict):
        self.data_dict = data_dict
        super().__init__(args)
        

        self.swa_model = optim.swa_utils.AveragedModel(self.model)
        self.swa = args.swa
       

    def _build_model(self):
        # model input depends on data
        # train_data, train_loader = self._get_data(flag='TRAIN')
        test_data, test_loader = self._get_data(self.data_dict,flag="TEST")
        self.args.seq_len = test_data.max_seq_len  # redefine seq_len
        self.args.pred_len = 0
        # self.args.enc_in = train_data.feature_df.shape[1]
        # self.args.num_class = len(train_data.class_names)
        self.args.enc_in = test_data.X.shape[2]  # redefine enc_in
        self.args.num_class = len(np.unique(test_data.y.cpu()))
        if self.args.data == "mitecg":
            self.args.num_class = 2
        
        
        # model init
        model = (
            self.model_dict[self.args.model].Model(self.args).float()
        )  # pass args to model
        if self.args.use_multi_gpu and self.args.use_gpu:
            model = nn.DataParallel(model, device_ids=self.args.device_ids)
            
        print("self.args.seq_len", self.args.seq_len)
        return model

    def _get_data(self, data_dict,flag):
        random.seed(self.args.seed)
        data_set=data_dict[flag] 
        data_loader =  DataLoader(data_set, batch_size=self.args.batch_size, shuffle=True)
        return data_set, data_loader

    def _select_optimizer(self):
        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)
        return model_optim

    def _select_criterion(self):
        criterion = nn.CrossEntropyLoss()
        return criterion

    def vali(self, vali_data, vali_loader, criterion):
        total_loss = []
        preds = []
        trues = []
        if self.swa:
            self.swa_model.eval()
        else:
            self.model.eval()
        with torch.no_grad():
            for i, (batch_x, label, padding_mask) in enumerate(vali_loader):
                batch_x = batch_x.float().to(self.device)
                padding_mask = padding_mask.float().to(self.device)
                label = label.to(self.device)

                if self.swa:
                    outputs = self.swa_model(batch_x, padding_mask, None, None)
                else:
                    outputs , place_holder,_= self.model(batch_x, padding_mask, None, None)

                pred = outputs.detach().cpu()
                loss = criterion(pred, label.cpu())
                total_loss.append(loss)

                preds.append(outputs.detach())
                trues.append(label)

        total_loss = np.average(total_loss)

        preds = torch.cat(preds, 0)
        trues = torch.cat(trues, 0)
        probs = torch.nn.functional.softmax(
            preds
        )  # (total_samples, num_classes) est. prob. for each class and sample
        trues_onehot = (
            torch.nn.functional.one_hot(
                trues.reshape(
                    -1,
                ).to(torch.long),
                num_classes=self.args.num_class,
            )
            .float()
            .cpu()
            .numpy()
        )
        # print(trues_onehot.shape)
        predictions = (
            torch.argmax(probs, dim=1).cpu().numpy()
        )  # (total_samples,) int class index for each sample
        probs = probs.cpu().numpy()
        trues = trues.flatten().cpu().numpy()
        # accuracy = cal_accuracy(predictions, trues)
        metrics_dict = {
            "Accuracy": accuracy_score(trues, predictions),
            "Precision": precision_score(trues, predictions, average="macro"),
            "Recall": recall_score(trues, predictions, average="macro"),
            "F1": f1_score(trues, predictions, average="macro"),
            "AUROC": roc_auc_score(trues_onehot, probs, multi_class="ovr"),
            "AUPRC": average_precision_score(trues_onehot, probs, average="macro"),
        }

        if self.swa:
            self.swa_model.train()
        else:
            self.model.train()
        return total_loss, metrics_dict

    def train(self, setting):
        train_data, train_loader = self._get_data(self.data_dict,flag="TRAIN")
        vali_data, vali_loader = self._get_data(self.data_dict,flag="VAL")
        test_data, test_loader = self._get_data(self.data_dict,flag="TEST")
        
        print(train_data.X.shape)
        print(train_data.y.shape)
        print(vali_data.X.shape)
        print(vali_data.y.shape)
        print(test_data.X.shape)
        print(test_data.y.shape)

        path = (
            "./seg_checkpoints/"
            + setting
            + "/"
        )
        if not os.path.exists(path):
            os.makedirs(path)

        time_now = time.time()

        train_steps = len(train_loader)
        early_stopping = EarlyStopping(
            patience=self.args.patience, verbose=True, delta=1e-5
        )

        model_optim = self._select_optimizer()
        criterion = self._select_criterion()

        for epoch in range(self.args.train_epochs):
            iter_count = 0
            train_loss = []

            self.model.train()
            epoch_time = time.time()

            for i, (batch_x, label, padding_mask) in enumerate(train_loader):
                iter_count += 1
                model_optim.zero_grad()

                batch_x = batch_x.float().to(self.device)
                padding_mask = padding_mask.float().to(self.device)
                label = label.to(self.device)
                
                #print(f"batch_x: {batch_x.shape}, label: {label.shape}, padding_mask: {padding_mask.shape}")
                outputs, place_holder_1, prototype= self.model(batch_x, padding_mask, None, None)
                #print("!!!!!",outputs,"@",label)
                
                prototype_loss =  self.model.seg_prototype_loss ( place_holder_1,batch_x,prototype,outputs)
                
                
                    
                
                global place_holder 
                place_holder = place_holder_1
                
                activation_loss = self.model.activation_cosine_loss(place_holder_1)
                
                #print("outputs",outputs.shape,"label",label.shape)
                loss =  criterion(outputs, label)+ 10*prototype_loss # + activation_loss
                
            
                
                train_loss.append(loss.item())

                if (i + 1) % 100 == 0:
                    print("#"*10,"prototype_loss",prototype_loss,"loss",criterion(outputs, label),"#"*10)
                    
                    print(
                        "\titers: {0}, epoch: {1} | loss: {2:.7f}".format(
                            i + 1, epoch + 1, loss.item()
                        )
                    )
                    speed = (time.time() - time_now) / iter_count
                    left_time = speed * (
                        (self.args.train_epochs - epoch) * train_steps - i
                    )
                    print(
                        "\tspeed: {:.4f}s/iter; left time: {:.4f}s".format(
                            speed, left_time
                        )
                    )
                    iter_count = 0
                    time_now = time.time()

                loss.backward()
                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)
                model_optim.step()

            self.swa_model.update_parameters(self.model)

            print("Epoch: {} cost time: {}".format(epoch + 1, time.time() - epoch_time))
            train_loss = np.average(train_loss)
            vali_loss, val_metrics_dict = self.vali(vali_data, vali_loader, criterion)
            test_loss, test_metrics_dict = self.vali(test_data, test_loader, criterion)

            print(
                f"Epoch: {epoch + 1}, Steps: {train_steps}, | Train Loss: {train_loss:.5f}\n"
                f"Validation results --- Loss: {vali_loss:.5f}, "
                f"Accuracy: {val_metrics_dict['Accuracy']:.5f}, "
                f"Precision: {val_metrics_dict['Precision']:.5f}, "
                f"Recall: {val_metrics_dict['Recall']:.5f}, "
                f"F1: {val_metrics_dict['F1']:.5f}, "
                f"AUROC: {val_metrics_dict['AUROC']:.5f}, "
                f"AUPRC: {val_metrics_dict['AUPRC']:.5f}\n"
                f"Test results --- Loss: {test_loss:.5f}, "
                f"Accuracy: {test_metrics_dict['Accuracy']:.5f}, "
                f"Precision: {test_metrics_dict['Precision']:.5f}, "
                f"Recall: {test_metrics_dict['Recall']:.5f} "
                f"F1: {test_metrics_dict['F1']:.5f}, "
                f"AUROC: {test_metrics_dict['AUROC']:.5f}, "
                f"AUPRC: {test_metrics_dict['AUPRC']:.5f}\n"
            )
            early_stopping(
                -val_metrics_dict["F1"],
                self.swa_model if self.swa else self.model,
                path,
            )
            if early_stopping.early_stop:
                print("Early stopping")
                break
            """if (epoch + 1) % 5 == 0:
                adjust_learning_rate(model_optim, epoch + 1, self.args)"""

        best_model_path = path + "checkpoint.pth"
        if self.swa:
            self.swa_model.load_state_dict(torch.load(best_model_path),map_location=self.device)
        else:
            #self.model.load_state_dict(torch.load(best_model_path),map_location=self.device)
            state_dict = torch.load(best_model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)

        return self.model

    def test(self, setting, test=0):
        vali_data, vali_loader = self._get_data(self.data_dict,flag="VAL")
        test_data, test_loader = self._get_data(self.data_dict,flag="TEST")
        if test:
            print("loading model")
            
            path = (
            "./seg_checkpoints/"
            + setting
            + "/"
        )

            model_path = path + "checkpoint.pth"
            if not os.path.exists(model_path):
                raise Exception("No model found at %s" % model_path)
            if self.swa:
                self.swa_model.load_state_dict(torch.load(model_path))
            else:
                self.model.load_state_dict(torch.load(model_path))

        criterion = self._select_criterion()
        vali_loss, val_metrics_dict = self.vali(vali_data, vali_loader, criterion)
        test_loss, test_metrics_dict = self.vali(test_data, test_loader, criterion)

        # result save
        folder_path = (
            "./results/"
            + self.args.task_name
            + "/"
            + self.args.model_id
            + "/"
            + self.args.model
            + "/"
        )
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        print(
            f"Validation results --- Loss: {vali_loss:.5f}, "
            f"Accuracy: {val_metrics_dict['Accuracy']:.5f}, "
            f"Precision: {val_metrics_dict['Precision']:.5f}, "
            f"Recall: {val_metrics_dict['Recall']:.5f}, "
            f"F1: {val_metrics_dict['F1']:.5f}, "
            f"AUROC: {val_metrics_dict['AUROC']:.5f}, "
            f"AUPRC: {val_metrics_dict['AUPRC']:.5f}\n"
            f"Test results --- Loss: {test_loss:.5f}, "
            f"Accuracy: {test_metrics_dict['Accuracy']:.5f}, "
            f"Precision: {test_metrics_dict['Precision']:.5f}, "
            f"Recall: {test_metrics_dict['Recall']:.5f}, "
            f"F1: {test_metrics_dict['F1']:.5f}, "
            f"AUROC: {test_metrics_dict['AUROC']:.5f}, "
            f"AUPRC: {test_metrics_dict['AUPRC']:.5f}\n"
        )
        file_name = "result_classification.txt"
        f = open(os.path.join(folder_path, file_name), "a")
        f.write(setting + "  \n")
        f.write(
            f"Validation results --- Loss: {vali_loss:.5f}, "
            f"Accuracy: {val_metrics_dict['Accuracy']:.5f}, "
            f"Precision: {val_metrics_dict['Precision']:.5f}, "
            f"Recall: {val_metrics_dict['Recall']:.5f}, "
            f"F1: {val_metrics_dict['F1']:.5f}, "
            f"AUROC: {val_metrics_dict['AUROC']:.5f}, "
            f"AUPRC: {val_metrics_dict['AUPRC']:.5f}\n"
            f"Test results --- Loss: {test_loss:.5f}, "
            f"Accuracy: {test_metrics_dict['Accuracy']:.5f}, "
            f"Precision: {test_metrics_dict['Precision']:.5f}, "
            f"Recall: {test_metrics_dict['Recall']:.5f}, "
            f"F1: {test_metrics_dict['F1']:.5f}, "
            f"AUROC: {test_metrics_dict['AUROC']:.5f}, "
            f"AUPRC: {test_metrics_dict['AUPRC']:.5f}\n"
        )
        f.write("\n")
        f.write("\n")
        f.close()
        return



def get_class_model(args,X):
    
    
    if args.class_model_type=="cnn":
        class_model = CNN(
        d_inp = X[0].shape[-1],
        n_classes = 2,)
        
    elif args.class_model_type=="lstm":
            class_model = LSTM(
        d_inp = X[0].shape[-1],
        n_classes = 2,
    )
            
    else:
        class_model=TransformerMVTS(
     
                 d_inp = X.shape[-1],
                max_len = X.shape[0],
                nlayers = 1,
                n_classes = 2,
                trans_dim_feedforward = 64,
                trans_dropout = 0.1,
                d_pe = 16,
                stronger_clf_head = False,
                norm_embedding = True,
                    )
    
    
    
    
    if args.data == "mitecg":
        
        
        



        class_model.load_state_dict(torch.load(f"/home/hbs/TS/XTS/TimeX/models/mitecg_{args.class_model_type}_split=1.pt",map_location=args.device))

        class_model=class_model.to(args.device)
        class_model.eval()
    
    
    # check if data start from "freqshape":
    if args.data.rsplit("_", 1)[0] == "freqshape":
        split_no = int(args.data.rsplit("_", 1)[1])
        
        
        class_model= TransformerMVTS(
                d_inp = X.shape[-1],
                max_len = X.shape[0],
                n_classes = 4,
                trans_dim_feedforward = 16,
                trans_dropout = 0.1,
                d_pe = 16,)
        class_model.load_state_dict(torch.load(f"/home/hbs/TS/XTS/TimeX/models/freqshape_transformer_split={split_no}.pt",map_location=args.device))
        class_model=class_model.to(args.device)
        
        
    return class_model





def train_seg_model(args):
    pass
    # get data
    
def vis_activation(args,test_dataset,model):
    dataloader=DataLoader(test_dataset, batch_size=20, shuffle=False)
    batch = next(iter(dataloader))


    #model = ProtoPTST.Model(args).float().to(args.device)
                        
                        # 计算第 500 个元素在哪个批次和批次中的位置
    target_index = 310  # 第  个元素对应索引  ！！2245
    batch_size = 20
    batch_index = target_index // batch_size  # 所在批次的索引
    element_index = target_index % batch_size  # 批次内的索引

    # 遍历批次直到找到包含第 500 个元素的批次
    for i, batch in enumerate(dataloader):
        if i == batch_index:
            element_500 = batch
            #print(element_500)
            break
        
    batch=element_500

    out,actions,prototype =model(batch[0],0,0,0)

    out,actions,prototype =model(batch[0],0,0,0)

    choice=2
    x_show=batch[0][choice].reshape(-1).cpu()     



    #print(place_holder.shape)
    for choice in [4]:
        
        x_show=batch[0][choice].reshape(-1).cpu()

        plt.figure(figsize=(15, 5))
        plt.plot(x_show.cpu().numpy(), color='black',label="Input TS",linewidth=5)
        plt.title(f" Dataset: {args.data}: class{batch[1][choice]},  Prototype_num: {args.num_prototypes},  Prototype_len: {args.prototype_len}",fontsize=20)
        for i in range(args.num_prototypes):
            line=actions[choice,:,i].reshape(-1).cpu().detach().numpy()
            

            
            # 设置画布大小为 10 x 5 英寸

            plt.plot(line,label=f"Activations_{i}",linewidth=1)

        plt.legend()
        save_path="./vis/activation"

        plt.savefig(save_path+f"_{choice}.png")
        
        
        
        ##
        plt.figure(figsize=(5, 6))
        # set backgroud to be opaque:



        #x_0=[0]*proto_len
        #plt.plot(x_0, color='r')
        for i in range(args.num_prototypes):
            line=prototype[i,:,:].reshape(-1).cpu().detach().numpy()
            #line=line+8*i
            plt.plot(line,label=f"Shapelet_{i}",linewidth=10,alpha=0.6)
            #line=prototype.reshape(-1).cpu().detach().numpy()

        # set legend out of the plot,and set the size of the legend
        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize=30)


            #plt.plot(line, color='g',linewidth=3)

        plt.title(f"Leaned shaplets in {args.data}") # , without class loss

        plt.savefig(f"./vis/shapelets.png", bbox_inches='tight')

        plt.show()

    


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # Experiment settings   
    # Task settings
    parser.add_argument("--task_name", type=str, default="classification")
    parser.add_argument("--is_training", type=int, default=1)
    parser.add_argument("--model_id", type=str, default="dataset_name")  # Replace 'dataset_name' with the actual variable value
    parser.add_argument("--model", type=str, default="ProtoPTST")

    # Data settings
    parser.add_argument("--data", type=str, default="dataset_name")  # Replace 'dataset_name' with the actual variable value
    parser.add_argument("--root_path", type=str, default="/home/hbs/TS/XTS/TimeX/datasets/SeqCombSingle")
    parser.add_argument("--two_class", type=int, default=1)
    parser.add_argument("--saliency", type=bool, default=False)
    parser.add_argument("--s_no", type=int, default=0)


    # Model settings for classification
    parser.add_argument("--seq_len", type=int, default=96)
    parser.add_argument("--e_layers", type=int, default=6)
    parser.add_argument("--d_model", type=int, default=128)
    parser.add_argument("--d_ff", type=int, default=256)
    parser.add_argument("--dec_in", type=int, default=7)
    parser.add_argument("--c_out", type=int, default=7)
    parser.add_argument("--n_heads", type=int, default=8)
    parser.add_argument("--top_k", type=int, default=5)
    parser.add_argument("--num_kernels", type=int, default=6)
    parser.add_argument("--moving_avg", type=int, default=25)
    parser.add_argument("--factor", type=int, default=1)
    parser.add_argument("--distil", type=bool, default=True)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--embed", type=str, default="timeF")
    parser.add_argument("--activation", type=str, default="gelu")
    parser.add_argument("--output_attention", type=bool, default=True)
    parser.add_argument("--no_inter_attn", type=bool, default=False)
    parser.add_argument("--chunk_size", type=int, default=16)
    parser.add_argument("--patch_len", type=int, default=4)
    parser.add_argument("--stride", type=int, default=1)
    parser.add_argument("--sampling_rate", type=int, default=256)
    parser.add_argument("--patch_len_list", type=list, default=[2, 4, 8])
    parser.add_argument("--single_channel", type=bool, default=False)
    parser.add_argument("--augmentations", type=str, default="flip,shuffle,jitter,mask,drop")

    # Optimization settings
    parser.add_argument("--batch_size", type=int, default=16)  # Replace 'BATCH_SIZE' with the actual variable value
    parser.add_argument("--train_epochs", type=int, default=10) # 100
    parser.add_argument("--learning_rate", type=float, default=0.0001)
    parser.add_argument("--itr", type=int, default=1)
    parser.add_argument("--patience", type=int, default=3) # 5
    parser.add_argument("--des", type=str, default="Exp")
    parser.add_argument("--loss", type=str, default="MSE")
    parser.add_argument("--lradj", type=str, default="type1")
    parser.add_argument("--use_amp", type=bool, default=False)
    parser.add_argument("--swa", type=bool, default=False)
    parser.add_argument("--d_layers", type=int, default=1)

    # GPU settings
    parser.add_argument("--use_gpu", type=bool, default=True)
    parser.add_argument("--gpu", type=str, default=1)  # Replace 'gpu_n' with the actual variable value
    parser.add_argument("--use_multi_gpu", type=bool, default=False)
    parser.add_argument("--devices", type=str, default=1)  # Replace 'gpu_n' with the actual variable value
    parser.add_argument("--device", type=str, default="cuda:1")  # Replace 'gpu_n' with the actual variable value

    # De-stationary projector parameters
    parser.add_argument("--p_hidden_dims", type=list, default=[128, 128])
    parser.add_argument("--p_hidden_layers", type=int, default=2)
    parser.add_argument("--freq", type=str, default="h")
    parser.add_argument("--num_workers", type=int, default=1)

    # Building model args
    parser.add_argument("--pred_len", type=int, default=0)
    parser.add_argument("--enc_in", type=int, default=1)  # Replace 'CHANNEL' with the actual variable value
    parser.add_argument("--num_class", type=int, default=2)  # Replace 'CLASS_NUMBER' with the actual variable value

    # ProtoPTST args
    parser.add_argument("--num_prototypes", "-n_pro", type=int)  # Replace 'PROTO_NUM' with the actual variable value
    parser.add_argument("--prototype_len","-pro_len",type=int)  # Replace 'proto_len' with the actual variable value
    parser.add_argument("--prototype_init",default= "kaiming",type=str)
    parser.add_argument("--prototype_activation",default= "linear",type=str)
    parser.add_argument("--ablation",default= "none",type=str,help=["no_matching_loss","no_variances_loss",'no_prototype_layer'])
    parser.add_argument("--class_model_type",default="cnn")
    parser.add_argument("--equal_seg_len",default=0, type= int)
    parser.add_argument("--seq_method",default=None, type= str)
    ############ Parse arguments
    args = parser.parse_args()
    ## print(args)
    print("#"*20)
    print(args)
    print("#"*20)
    ############ Load data
    data_dict = get_saliency_data(args)  # Add your data loading logic here
    
    
    ############ Get settings
    
    trained_model=None
    setting = f"class_model{args.class_model_type}_{args.ablation}_{args.data}_slen{args.seq_len}_npro{args.num_prototypes}_prototype_len{args.prototype_len}_epoch{args.train_epochs}"
    
    if args.is_training:
        for ii in range(args.itr):
            seed = 41 + ii
            random.seed(seed)
            os.environ["PYTHONHASHSEED"] = str(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            # comment out the following lines if you are using dilated convolutions, e.g., TCN
            # otherwise it will slow down the training extremely
            torch.backends.cudnn.benchmark = False
            torch.backends.cudnn.deterministic = True


            # setting record of experiments
            args.seed = seed
            
            ############ Train segmentation model
            exp = Exp_Classification(args, data_dict)
            exp.train(setting)
            
            model=exp.model
            trained_model=model
            
        
    
    else:
        
            

        #seg_model = ProtoPTST.Model(args).float()
        seg_model = ProtoPTST.Model(args).float()
       
        seg_model.load_state_dict(torch.load(f"/home/hbs/TS/XTS/TimeX/ShapeX/seg_checkpoints/{setting}/checkpoint.pth"))
        
        
        seg_model=seg_model.to(args.device)
        trained_model=seg_model
        
        
    ############ Load test data and classification model
    
    seg_model =trained_model
        
    
    args.is_training = 0

    args.saliency = True
    
    X ,y,times, gt_exps = get_saliency_data(args)
    X, y, times, gt_exps = X.cpu(), y.cpu(), times.cpu(), gt_exps.cpu()
    
    
    
    
    if False:
        X = frac_seires(X)
        gt_exps = frac_seires(gt_exps)
    
    class_model = get_class_model(args,X)
    
    ############ compute saliency scores
    Scorer = ScoreSubsequences(args,class_model, seg_model)
    score = Scorer.get_score_as_GTEXP(X).unsqueeze(-1)
 
    
    print("#"*10,score.shape)
    print("#"*10,gt_exps.shape)
    # save score  in txt file
    np.savetxt("score.txt", score.squeeze(-1).cpu().numpy())
    np.savetxt("gt_exps.txt", gt_exps.squeeze(-1).cpu().numpy())
    
    for i in range(1):
        plot_visualize_some(X, score, gt_exps, times, y, choice=i, saving_path="./vis", if_norm=False, mo_s=None)
    
    vis_activation(args,data_dict["TEST"],seg_model)
    
    
    results_dict = ground_truth_xai_eval(score, gt_exps)
    #print(results_dict)
    for k, v in results_dict.items():
        print('\t{} \t = {:.4f} +- {:.4f}'.format(k, np.mean(v), np.std(v) / np.sqrt(len(v))))
        
    
    with open("results.txt", "a") as f:  # "a" 表示追加模式
        
        if setting is not None:
            f.write(setting)
        else:
            f.write("mitecg_mitecg_slen96_npro4_prototype_len30_seed41 ")
        for k, v in results_dict.items():
            f.write(f' {k}:{np.mean(v):.4f}-{(np.std(v) / np.sqrt(len(v))):.4f}')
        f.write("\n") 
